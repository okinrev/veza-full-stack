---
title: Profilage de Performance
sidebar_label: Profilage
---

# üöÄ Profilage de Performance

Ce guide explique comment profiler les performances sur Veza.

## Vue d'ensemble

Ce guide d√©taille les techniques et outils de profiling de performance pour la plateforme Veza, couvrant l'analyse CPU, m√©moire, r√©seau et les optimisations.

## Table des mati√®res

- [M√©triques de Performance](#m√©triques-de-performance)
- [Outils de Profiling](#outils-de-profiling)
- [Techniques d'Optimisation](#techniques-doptimisation)
- [Bonnes Pratiques](#bonnes-pratiques)
- [Pi√®ges √† √âviter](#pi√®ges-√†-√©viter)
- [Ressources](#ressources)

## M√©triques de Performance

### 1. M√©triques Syst√®me

```yaml
# performance-profiling/metrics/system-metrics.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: veza-system-metrics
  namespace: veza
data:
  # M√©triques CPU
  cpu_metrics:
    - "CPU Usage: Pourcentage d'utilisation"
    - "CPU Load: Charge moyenne"
    - "CPU Time: Temps CPU par processus"
    - "Context Switches: Changements de contexte"
  
  # M√©triques M√©moire
  memory_metrics:
    - "Memory Usage: Utilisation m√©moire"
    - "Memory Allocation: Allocations par seconde"
    - "Garbage Collection: Fr√©quence et dur√©e"
    - "Memory Leaks: Fuites m√©moire"
  
  # M√©triques R√©seau
  network_metrics:
    - "Network I/O: Entr√©es/sorties r√©seau"
    - "Connection Pool: Pool de connexions"
    - "Latency: Latence r√©seau"
    - "Throughput: D√©bit r√©seau"
  
  # M√©triques Disque
  disk_metrics:
    - "Disk I/O: Entr√©es/sorties disque"
    - "Disk Space: Espace disque"
    - "Disk Latency: Latence disque"
    - "File Operations: Op√©rations fichiers"
```

### 2. M√©triques Application

```go
// performance-profiling/metrics/app-metrics.go
package metrics

import (
    "runtime"
    "time"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

// M√©triques Prometheus
var (
    // M√©triques HTTP
    httpRequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total des requ√™tes HTTP",
        },
        []string{"method", "endpoint", "status"},
    )

    httpRequestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "http_request_duration_seconds",
            Help:    "Dur√©e des requ√™tes HTTP",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )

    // M√©triques Base de donn√©es
    dbQueryDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "db_query_duration_seconds",
            Help:    "Dur√©e des requ√™tes base de donn√©es",
            Buckets: prometheus.DefBuckets,
        },
        []string{"query_type", "table"},
    )

    // M√©triques M√©moire
    memoryAllocations = promauto.NewGauge(
        prometheus.GaugeOpts{
            Name: "memory_allocations_bytes",
            Help: "Allocations m√©moire en bytes",
        },
    )

    // M√©triques Goroutines
    goroutinesCount = promauto.NewGauge(
        prometheus.GaugeOpts{
            Name: "goroutines_count",
            Help: "Nombre de goroutines actives",
        },
    )
)

// Collecteur de m√©triques syst√®me
type SystemMetricsCollector struct {
    lastCollection time.Time
}

func NewSystemMetricsCollector() *SystemMetricsCollector {
    return &SystemMetricsCollector{
        lastCollection: time.Now(),
    }
}

func (c *SystemMetricsCollector) Collect() {
    // M√©triques m√©moire
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    memoryAllocations.Set(float64(m.Alloc))

    // M√©triques goroutines
    goroutinesCount.Set(float64(runtime.NumGoroutine()))

    c.lastCollection = time.Now()
}

// Middleware pour m√©triques HTTP
func MetricsMiddleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()

        // Wrapper pour capturer le statut
        rw := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}
        next.ServeHTTP(rw, r)

        // Enregistrement des m√©triques
        duration := time.Since(start).Seconds()
        httpRequestsTotal.WithLabelValues(r.Method, r.URL.Path, string(rw.statusCode)).Inc()
        httpRequestDuration.WithLabelValues(r.Method, r.URL.Path).Observe(duration)
    })
}

type responseWriter struct {
    http.ResponseWriter
    statusCode int
}

func (rw *responseWriter) WriteHeader(code int) {
    rw.statusCode = code
    rw.ResponseWriter.WriteHeader(code)
}
```

## Outils de Profiling

### 1. Configuration pprof

```go
// performance-profiling/pprof/pprof-server.go
package pprof

import (
    "net/http"
    _ "net/http/pprof"
    "runtime"
    "time"
)

// Serveur pprof
func StartPprofServer(addr string) {
    go func() {
        log.Printf("Pprof server started on %s", addr)
        log.Fatal(http.ListenAndServe(addr, nil))
    }()
}

// Profiling CPU
func StartCPUProfiling(duration time.Duration) {
    f, err := os.Create("cpu.prof")
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()

    if err := pprof.StartCPUProfile(f); err != nil {
        log.Fatal(err)
    }

    time.Sleep(duration)
    pprof.StopCPUProfile()
}

// Profiling M√©moire
func StartMemoryProfiling() {
    f, err := os.Create("memory.prof")
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()

    if err := pprof.WriteHeapProfile(f); err != nil {
        log.Fatal(err)
    }
}

// Profiling Goroutines
func StartGoroutineProfiling() {
    f, err := os.Create("goroutine.prof")
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()

    if err := pprof.Lookup("goroutine").WriteTo(f, 0); err != nil {
        log.Fatal(err)
    }
}
```

### 2. Scripts de Profiling

```bash
#!/bin/bash
# performance-profiling/scripts/profile.sh

# Configuration
PROFILE_DURATION=30
PROFILE_OUTPUT="profiles"
SERVICE_URL="http://localhost:8080"

# Cr√©ation du r√©pertoire de sortie
mkdir -p $PROFILE_OUTPUT

# Profiling CPU
cpu_profile() {
    echo "D√©marrage du profiling CPU..."
    
    # D√©marrage du profiling
    curl -X POST http://localhost:6060/debug/pprof/profile?seconds=$PROFILE_DURATION \
        -o $PROFILE_OUTPUT/cpu.prof
    
    echo "Profiling CPU termin√©: $PROFILE_OUTPUT/cpu.prof"
}

# Profiling M√©moire
memory_profile() {
    echo "D√©marrage du profiling m√©moire..."
    
    # Profiling heap
    curl http://localhost:6060/debug/pprof/heap \
        -o $PROFILE_OUTPUT/heap.prof
    
    # Profiling allocations
    curl http://localhost:6060/debug/pprof/allocs \
        -o $PROFILE_OUTPUT/allocs.prof
    
    echo "Profiling m√©moire termin√©"
}

# Profiling Goroutines
goroutine_profile() {
    echo "D√©marrage du profiling goroutines..."
    
    curl http://localhost:6060/debug/pprof/goroutine \
        -o $PROFILE_OUTPUT/goroutine.prof
    
    echo "Profiling goroutines termin√©"
}

# Analyse des profils
analyze_profiles() {
    echo "Analyse des profils..."
    
    # Analyse CPU
    if [ -f "$PROFILE_OUTPUT/cpu.prof" ]; then
        echo "=== Analyse CPU ==="
        go tool pprof -top $PROFILE_OUTPUT/cpu.prof
        echo ""
    fi
    
    # Analyse M√©moire
    if [ -f "$PROFILE_OUTPUT/heap.prof" ]; then
        echo "=== Analyse M√©moire ==="
        go tool pprof -top $PROFILE_OUTPUT/heap.prof
        echo ""
    fi
    
    # Analyse Goroutines
    if [ -f "$PROFILE_OUTPUT/goroutine.prof" ]; then
        echo "=== Analyse Goroutines ==="
        go tool pprof -top $PROFILE_OUTPUT/goroutine.prof
        echo ""
    fi
}

# Test de charge
load_test() {
    echo "Test de charge..."
    
    # Test avec wrk
    if command -v wrk &> /dev/null; then
        wrk -t4 -c100 -d30s $SERVICE_URL/health
    else
        echo "wrk non install√©, utilisation d'ab"
        ab -n 1000 -c 10 $SERVICE_URL/health
    fi
}

# Menu principal
case "${1:-}" in
    cpu)
        cpu_profile
        ;;
    memory)
        memory_profile
        ;;
    goroutine)
        goroutine_profile
        ;;
    all)
        cpu_profile
        memory_profile
        goroutine_profile
        ;;
    analyze)
        analyze_profiles
        ;;
    load)
        load_test
        ;;
    *)
        echo "Usage: $0 {cpu|memory|goroutine|all|analyze|load}"
        exit 1
        ;;
esac
```

## Techniques d'Optimisation

### 1. Optimisation CPU

```go
// performance-profiling/optimization/cpu-optimization.go
package optimization

import (
    "runtime"
    "sync"
    "time"
)

// Optimisation du pool de workers
type WorkerPool struct {
    workers    int
    jobQueue   chan Job
    workerPool chan chan Job
    quit       chan bool
    wg         sync.WaitGroup
}

type Job struct {
    ID       int
    Data     interface{}
    Process  func(interface{}) error
}

func NewWorkerPool(workers int) *WorkerPool {
    return &WorkerPool{
        workers:    workers,
        jobQueue:   make(chan Job, 1000),
        workerPool: make(chan chan Job, workers),
        quit:       make(chan bool),
    }
}

func (wp *WorkerPool) Start() {
    for i := 0; i < wp.workers; i++ {
        wp.wg.Add(1)
        go wp.worker()
    }
    
    go wp.dispatcher()
}

func (wp *WorkerPool) worker() {
    defer wp.wg.Done()
    
    jobChannel := make(chan Job)
    wp.workerPool <- jobChannel
    
    for {
        select {
        case job := <-jobChannel:
            job.Process(job.Data)
            wp.workerPool <- jobChannel
        case <-wp.quit:
            return
        }
    }
}

func (wp *WorkerPool) dispatcher() {
    for {
        select {
        case job := <-wp.jobQueue:
            go func(job Job) {
                jobChannel := <-wp.workerPool
                jobChannel <- job
            }(job)
        case <-wp.quit:
            return
        }
    }
}

func (wp *WorkerPool) Submit(job Job) {
    wp.jobQueue <- job
}

func (wp *WorkerPool) Stop() {
    close(wp.quit)
    wp.wg.Wait()
}

// Optimisation du cache
type Cache struct {
    data map[string]interface{}
    mu   sync.RWMutex
    ttl  time.Duration
}

func NewCache(ttl time.Duration) *Cache {
    cache := &Cache{
        data: make(map[string]interface{}),
        ttl:  ttl,
    }
    
    // Nettoyage p√©riodique
    go cache.cleanup()
    
    return cache
}

func (c *Cache) Set(key string, value interface{}) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    c.data[key] = value
}

func (c *Cache) Get(key string) (interface{}, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()
    
    value, exists := c.data[key]
    return value, exists
}

func (c *Cache) cleanup() {
    ticker := time.NewTicker(c.ttl)
    defer ticker.Stop()
    
    for range ticker.C {
        c.mu.Lock()
        // Nettoyage des entr√©es expir√©es
        for key := range c.data {
            // Logique de nettoyage bas√©e sur TTL
            delete(c.data, key)
        }
        c.mu.Unlock()
    }
}
```

### 2. Optimisation M√©moire

```go
// performance-profiling/optimization/memory-optimization.go
package optimization

import (
    "sync"
    "time"
)

// Pool d'objets pour r√©duire les allocations
type ObjectPool struct {
    pool sync.Pool
}

func NewObjectPool(newFunc func() interface{}) *ObjectPool {
    return &ObjectPool{
        pool: sync.Pool{
            New: newFunc,
        },
    }
}

func (op *ObjectPool) Get() interface{} {
    return op.pool.Get()
}

func (op *ObjectPool) Put(obj interface{}) {
    op.pool.Put(obj)
}

// Optimisation des slices
type OptimizedSlice struct {
    data []interface{}
    mu   sync.RWMutex
}

func NewOptimizedSlice(capacity int) *OptimizedSlice {
    return &OptimizedSlice{
        data: make([]interface{}, 0, capacity),
    }
}

func (os *OptimizedSlice) Add(item interface{}) {
    os.mu.Lock()
    defer os.mu.Unlock()
    
    os.data = append(os.data, item)
}

func (os *OptimizedSlice) Get(index int) (interface{}, bool) {
    os.mu.RLock()
    defer os.mu.RUnlock()
    
    if index < len(os.data) {
        return os.data[index], true
    }
    return nil, false
}

// Optimisation des maps
type OptimizedMap struct {
    data map[string]interface{}
    mu   sync.RWMutex
}

func NewOptimizedMap() *OptimizedMap {
    return &OptimizedMap{
        data: make(map[string]interface{}),
    }
}

func (om *OptimizedMap) Set(key string, value interface{}) {
    om.mu.Lock()
    defer om.mu.Unlock()
    
    om.data[key] = value
}

func (om *OptimizedMap) Get(key string) (interface{}, bool) {
    om.mu.RLock()
    defer om.mu.RUnlock()
    
    value, exists := om.data[key]
    return value, exists
}
```

## Bonnes Pratiques

### 1. R√®gles de Performance

```yaml
# performance-profiling/best-practices/performance-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: veza-performance-rules
  namespace: veza
data:
  # R√®gles CPU
  cpu_rules:
    - "Utiliser des goroutines pour la concurrence"
    - "√âviter les boucles infinies"
    - "Optimiser les algorithmes critiques"
    - "Utiliser des pools de workers"
    - "Profiler r√©guli√®rement"
  
  # R√®gles M√©moire
  memory_rules:
    - "R√©utiliser les objets avec sync.Pool"
    - "Pr√©-allouer les slices et maps"
    - "√âviter les allocations dans les boucles"
    - "Utiliser des pointeurs pour les gros objets"
    - "Surveiller les fuites m√©moire"
  
  # R√®gles R√©seau
  network_rules:
    - "Utiliser des connexions persistantes"
    - "Impl√©menter du connection pooling"
    - "Optimiser les requ√™tes base de donn√©es"
    - "Utiliser la compression"
    - "Impl√©menter du caching"
  
  # R√®gles Base de donn√©es
  database_rules:
    - "Utiliser des index appropri√©s"
    - "Optimiser les requ√™tes"
    - "Utiliser des transactions"
    - "Impl√©menter du connection pooling"
    - "Surveiller les slow queries"
```

### 2. Monitoring de Performance

```go
// performance-profiling/monitoring/performance-monitor.go
package monitoring

import (
    "context"
    "runtime"
    "time"
)

// Moniteur de performance
type PerformanceMonitor struct {
    metrics map[string]float64
    mu      sync.RWMutex
}

func NewPerformanceMonitor() *PerformanceMonitor {
    pm := &PerformanceMonitor{
        metrics: make(map[string]float64),
    }
    
    // D√©marrage du monitoring
    go pm.monitor()
    
    return pm
}

func (pm *PerformanceMonitor) monitor() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        pm.collectMetrics()
    }
}

func (pm *PerformanceMonitor) collectMetrics() {
    pm.mu.Lock()
    defer pm.mu.Unlock()
    
    // M√©triques m√©moire
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    
    pm.metrics["memory_alloc"] = float64(m.Alloc)
    pm.metrics["memory_total_alloc"] = float64(m.TotalAlloc)
    pm.metrics["memory_sys"] = float64(m.Sys)
    pm.metrics["memory_num_gc"] = float64(m.NumGC)
    
    // M√©triques goroutines
    pm.metrics["goroutines"] = float64(runtime.NumGoroutine())
    
    // M√©triques CPU
    pm.metrics["cpu_cores"] = float64(runtime.NumCPU())
}

func (pm *PerformanceMonitor) GetMetrics() map[string]float64 {
    pm.mu.RLock()
    defer pm.mu.RUnlock()
    
    metrics := make(map[string]float64)
    for k, v := range pm.metrics {
        metrics[k] = v
    }
    
    return metrics
}
```

## Pi√®ges √† √âviter

### 1. Allocations Excessives

‚ùå **Mauvais** :
```go
// Allocations dans une boucle
for i := 0; i < 1000000; i++ {
    data := make([]byte, 1024)  // Allocation √† chaque it√©ration
    process(data)
}
```

‚úÖ **Bon** :
```go
// R√©utilisation d'objets
data := make([]byte, 1024)
for i := 0; i < 1000000; i++ {
    // R√©utilisation du m√™me slice
    process(data)
}
```

### 2. Goroutines Non Contr√¥l√©es

‚ùå **Mauvais** :
```go
// Goroutines non contr√¥l√©es
for i := 0; i < 1000000; i++ {
    go process(i)  // Cr√©ation de trop de goroutines
}
```

‚úÖ **Bon** :
```go
// Pool de workers
pool := NewWorkerPool(100)
pool.Start()

for i := 0; i < 1000000; i++ {
    pool.Submit(Job{ID: i, Process: process})
}
```

### 3. Pas de Monitoring

‚ùå **Mauvais** :
```go
// Pas de monitoring
func process() {
    // Logique sans monitoring
}
```

‚úÖ **Bon** :
```go
// Avec monitoring
func process() {
    start := time.Now()
    defer func() {
        duration := time.Since(start)
        metrics.RecordDuration("process", duration)
    }()
    
    // Logique avec monitoring
}
```

## Ressources

### Documentation Interne

- [Guide de Monitoring](../monitoring/README.md)
- [Guide de Debugging](./debugging.md)
- [Guide de Tests](../testing/README.md)

### Outils Recommand√©s

- **pprof** : Profiling Go
- **perf** : Profiling syst√®me
- **Prometheus** : M√©triques
- **Grafana** : Visualisation

### Commandes Utiles

```bash
# Profiling CPU
go tool pprof http://localhost:6060/debug/pprof/profile

# Profiling M√©moire
go tool pprof http://localhost:6060/debug/pprof/heap

# Analyse de performance
go test -bench=. -benchmem

# Monitoring en temps r√©el
htop
iotop
```

---

**Derni√®re mise √† jour** : $(date)
**Version du guide** : 1.0.0
**Mainteneur** : √âquipe Performance Veza 