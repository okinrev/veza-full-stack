/// Module Monitoring & Observabilit√© pour production
/// 
/// Impl√©mente m√©triques Prometheus, dashboards Grafana,
/// alerting intelligent et distributed tracing

pub mod prometheus_metrics;
pub mod grafana_dashboards;
pub mod alerting;
pub mod tracing;
pub mod health_checks;

pub use prometheus_metrics::*;
pub use grafana_dashboards::*;
pub use alerting::*;
pub use tracing::*;
pub use health_checks::*;

use std::sync::Arc;
use std::time::{Duration, SystemTime};
use tokio::sync::RwLock;
use tracing::{info, warn, error, debug};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

use crate::error::AppError;

/// Configuration du monitoring
#[derive(Debug, Clone)]
pub struct MonitoringConfig {
    /// Port pour serveur de m√©triques Prometheus
    pub metrics_port: u16,
    /// Interval de collecte des m√©triques
    pub collection_interval: Duration,
    /// Activation distributed tracing
    pub enable_tracing: bool,
    /// Endpoint Jaeger pour tracing
    pub jaeger_endpoint: Option<String>,
    /// Configuration alerting
    pub alerting_config: AlertingConfig,
    /// R√©tention des m√©triques
    pub metrics_retention: Duration,
}

/// Configuration d'alerting
#[derive(Debug, Clone)]
pub struct AlertingConfig {
    /// Webhook Slack pour alertes
    pub slack_webhook: Option<String>,
    /// Email pour alertes critiques
    pub alert_email: Option<String>,
    /// Seuils d'alerte
    pub thresholds: AlertThresholds,
}

/// Seuils d'alerte configurables
#[derive(Debug, Clone)]
pub struct AlertThresholds {
    /// Latence P99 critique (ms)
    pub critical_latency_p99_ms: f64,
    /// Taux d'erreur critique (%)
    pub critical_error_rate: f64,
    /// CPU critique (%)
    pub critical_cpu_usage: f64,
    /// M√©moire critique (%)
    pub critical_memory_usage: f64,
    /// Connexions minimum critique
    pub critical_min_connections: u32,
}

/// Gestionnaire principal du monitoring
#[derive(Debug)]
pub struct MonitoringManager {
    config: MonitoringConfig,
    prometheus_collector: Arc<PrometheusCollector>,
    grafana_manager: Arc<GrafanaManager>,
    alert_manager: Arc<AlertManager>,
    tracing_manager: Arc<TracingManager>,
    health_checker: Arc<HealthChecker>,
    metrics_store: Arc<RwLock<MetricsStore>>,
}

/// Store des m√©triques en m√©moire
#[derive(Debug, Default)]
pub struct MetricsStore {
    /// M√©triques syst√®me
    pub system_metrics: SystemMetrics,
    /// M√©triques application
    pub app_metrics: ApplicationMetrics,
    /// M√©triques business
    pub business_metrics: BusinessMetrics,
    /// Historique des alertes
    pub alert_history: Vec<AlertEvent>,
}

/// M√©triques syst√®me
#[derive(Debug, Clone, Default)]
pub struct SystemMetrics {
    pub cpu_usage_percent: f64,
    pub memory_usage_percent: f64,
    pub disk_usage_percent: f64,
    pub network_rx_bytes_per_sec: u64,
    pub network_tx_bytes_per_sec: u64,
    pub open_file_descriptors: u32,
    pub load_average_1m: f64,
    pub uptime_seconds: u64,
}

/// M√©triques application
#[derive(Debug, Clone, Default)]
pub struct ApplicationMetrics {
    pub active_connections: u32,
    pub requests_per_second: f64,
    pub latency_p50_ms: f64,
    pub latency_p95_ms: f64,
    pub latency_p99_ms: f64,
    pub error_rate_percent: f64,
    pub cache_hit_rate_percent: f64,
    pub database_connections: u32,
    pub queue_depth: u32,
}

/// M√©triques business
#[derive(Debug, Clone, Default)]
pub struct BusinessMetrics {
    pub total_streams: u32,
    pub active_streams: u32,
    pub total_listeners: u32,
    pub revenue_per_hour: f64,
    pub user_engagement_score: f64,
    pub conversion_rate_percent: f64,
    pub premium_users_percent: f64,
}

/// √âv√©nement d'alerte
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertEvent {
    pub id: Uuid,
    pub alert_type: AlertType,
    pub severity: AlertSeverity,
    pub message: String,
    pub triggered_at: SystemTime,
    pub resolved_at: Option<SystemTime>,
    pub affected_services: Vec<String>,
    pub metrics_snapshot: serde_json::Value,
}

/// Types d'alertes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AlertType {
    HighLatency,
    HighErrorRate,
    HighCpuUsage,
    HighMemoryUsage,
    ServiceDown,
    DatabaseIssue,
    NetworkIssue,
    SecurityIncident,
    BusinessMetricAnomaly,
}

/// S√©v√©rit√© des alertes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AlertSeverity {
    Info,
    Warning,
    Critical,
    Emergency,
}

impl Default for MonitoringConfig {
    fn default() -> Self {
        Self {
            metrics_port: 9090,
            collection_interval: Duration::from_secs(15),
            enable_tracing: true,
            jaeger_endpoint: Some("http://localhost:14268".to_string()),
            alerting_config: AlertingConfig {
                slack_webhook: None,
                alert_email: Some("alerts@veza.live".to_string()),
                thresholds: AlertThresholds {
                    critical_latency_p99_ms: 100.0,
                    critical_error_rate: 1.0,
                    critical_cpu_usage: 90.0,
                    critical_memory_usage: 85.0,
                    critical_min_connections: 1000,
                },
            },
            metrics_retention: Duration::from_secs(7 * 24 * 3600), // 7 jours
        }
    }
}

impl MonitoringManager {
    /// Cr√©e un nouveau gestionnaire de monitoring
    pub async fn new(config: MonitoringConfig) -> Result<Self, AppError> {
        info!("ÔøΩÔøΩ Initialisation Monitoring Manager");
        
        let prometheus_collector = Arc::new(PrometheusCollector::new().await?);
        let grafana_manager = Arc::new(GrafanaManager::new().await?);
        let alert_manager = Arc::new(AlertManager::new(config.alerting_config.clone()).await?);
        let tracing_manager = Arc::new(TracingManager::new(config.jaeger_endpoint.clone()).await?);
        let health_checker = Arc::new(HealthChecker::new().await?);
        
        Ok(Self {
            config,
            prometheus_collector,
            grafana_manager,
            alert_manager,
            tracing_manager,
            health_checker,
            metrics_store: Arc::new(RwLock::new(MetricsStore::default())),
        })
    }
    
    /// D√©marre tous les services de monitoring
    pub async fn start(&self) -> Result<(), AppError> {
        info!("üöÄ D√©marrage Monitoring & Observabilit√©");
        
        // D√©marrer collecteur Prometheus
        self.prometheus_collector.start(self.config.metrics_port).await?;
        
        // D√©marrer distributed tracing
        if self.config.enable_tracing {
            self.tracing_manager.start().await?;
        }
        
        // D√©marrer alerting
        self.alert_manager.start().await?;
        
        // D√©marrer health checks
        self.health_checker.start().await?;
        
        // D√©marrer collection p√©riodique des m√©triques
        self.start_metrics_collection().await;
        
        // D√©marrer monitoring des seuils d'alerte
        self.start_alert_monitoring().await;
        
        info!("‚úÖ Monitoring d√©marr√© avec succ√®s");
        Ok(())
    }
    
    /// Arr√™te tous les services de monitoring
    pub async fn stop(&self) -> Result<(), AppError> {
        info!("üõë Arr√™t Monitoring & Observabilit√©");
        
        self.health_checker.stop().await?;
        self.alert_manager.stop().await?;
        
        if self.config.enable_tracing {
            self.tracing_manager.stop().await?;
        }
        
        self.prometheus_collector.stop().await?;
        
        info!("‚úÖ Monitoring arr√™t√©");
        Ok(())
    }
    
    /// D√©marre la collection p√©riodique des m√©triques
    async fn start_metrics_collection(&self) {
        let metrics_store = self.metrics_store.clone();
        let interval = self.config.collection_interval;
        
        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(interval);
            
            loop {
                ticker.tick().await;
                
                // Collecter m√©triques syst√®me
                let system_metrics = Self::collect_system_metrics().await;
                
                // Collecter m√©triques application
                let app_metrics = Self::collect_application_metrics().await;
                
                // Collecter m√©triques business
                let business_metrics = Self::collect_business_metrics().await;
                
                // Stocker les m√©triques
                let mut store = metrics_store.write().await;
                store.system_metrics = system_metrics;
                store.app_metrics = app_metrics;
                store.business_metrics = business_metrics;
                
                debug!("üìä M√©triques collect√©es et stock√©es");
            }
        });
    }
    
    /// D√©marre le monitoring des seuils d'alerte
    async fn start_alert_monitoring(&self) {
        let metrics_store = self.metrics_store.clone();
        let alert_manager = self.alert_manager.clone();
        let thresholds = self.config.alerting_config.thresholds.clone();
        
        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(Duration::from_secs(30));
            
            loop {
                ticker.tick().await;
                
                let store = metrics_store.read().await;
                
                // V√©rifier seuils critiques
                if store.app_metrics.latency_p99_ms > thresholds.critical_latency_p99_ms {
                    let alert = AlertEvent {
                        id: Uuid::new_v4(),
                        alert_type: AlertType::HighLatency,
                        severity: AlertSeverity::Critical,
                        message: format!("Latence P99 critique: {:.1}ms", store.app_metrics.latency_p99_ms),
                        triggered_at: SystemTime::now(),
                        resolved_at: None,
                        affected_services: vec!["stream_server".to_string()],
                        metrics_snapshot: serde_json::json!({
                            "latency_p99": store.app_metrics.latency_p99_ms,
                            "active_connections": store.app_metrics.active_connections
                        }),
                    };
                    
                    if let Err(e) = alert_manager.send_alert(alert).await {
                        error!("‚ùå Erreur envoi alerte: {}", e);
                    }
                }
                
                // V√©rifier autres seuils...
                if store.app_metrics.error_rate_percent > thresholds.critical_error_rate {
                    warn!("‚ö†Ô∏è  Taux d'erreur √©lev√©: {:.2}%", store.app_metrics.error_rate_percent);
                }
                
                if store.system_metrics.cpu_usage_percent > thresholds.critical_cpu_usage {
                    warn!("‚ö†Ô∏è  CPU √©lev√©: {:.1}%", store.system_metrics.cpu_usage_percent);
                }
            }
        });
    }
    
    /// Collecte les m√©triques syst√®me
    async fn collect_system_metrics() -> SystemMetrics {
        // Simulation de collecte de m√©triques syst√®me
        SystemMetrics {
            cpu_usage_percent: 72.5,
            memory_usage_percent: 68.3,
            disk_usage_percent: 45.2,
            network_rx_bytes_per_sec: 1_250_000,
            network_tx_bytes_per_sec: 2_100_000,
            open_file_descriptors: 1024,
            load_average_1m: 2.8,
            uptime_seconds: 86400 * 5, // 5 jours
        }
    }
    
    /// Collecte les m√©triques application
    async fn collect_application_metrics() -> ApplicationMetrics {
        // Simulation de collecte de m√©triques application
        ApplicationMetrics {
            active_connections: 85_000,
            requests_per_second: 12_500.0,
            latency_p50_ms: 12.3,
            latency_p95_ms: 28.7,
            latency_p99_ms: 45.2,
            error_rate_percent: 0.08,
            cache_hit_rate_percent: 94.5,
            database_connections: 45,
            queue_depth: 128,
        }
    }
    
    /// Collecte les m√©triques business
    async fn collect_business_metrics() -> BusinessMetrics {
        // Simulation de collecte de m√©triques business
        BusinessMetrics {
            total_streams: 1_250,
            active_streams: 987,
            total_listeners: 125_000,
            revenue_per_hour: 1_850.75,
            user_engagement_score: 8.7,
            conversion_rate_percent: 3.2,
            premium_users_percent: 15.8,
        }
    }
    
    /// Obtient un snapshot des m√©triques actuelles
    pub async fn get_metrics_snapshot(&self) -> MetricsStore {
        self.metrics_store.read().await.clone()
    }
    
    /// G√©n√®re un rapport de sant√© complet
    pub async fn generate_health_report(&self) -> HealthReport {
        let metrics = self.get_metrics_snapshot().await;
        let health_status = self.health_checker.get_overall_health().await;
        
        HealthReport {
            timestamp: SystemTime::now(),
            overall_status: health_status,
            system_metrics: metrics.system_metrics,
            app_metrics: metrics.app_metrics,
            business_metrics: metrics.business_metrics,
            recent_alerts: metrics.alert_history.iter()
                .filter(|alert| {
                    alert.triggered_at
                        .duration_since(SystemTime::now() - Duration::from_secs(3600))
                        .is_ok()
                })
                .cloned()
                .collect(),
        }
    }
}

/// Rapport de sant√© complet
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthReport {
    pub timestamp: SystemTime,
    pub overall_status: HealthStatus,
    pub system_metrics: SystemMetrics,
    pub app_metrics: ApplicationMetrics,
    pub business_metrics: BusinessMetrics,
    pub recent_alerts: Vec<AlertEvent>,
}

/// Status de sant√© g√©n√©ral
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum HealthStatus {
    Healthy,
    Degraded { issues: Vec<String> },
    Unhealthy { critical_issues: Vec<String> },
}
